---
layout: post
title: Iceberg
categories: [数据湖]
description: 数据湖
keywords: 数据湖
---

# Iceberg

Apache Iceberg是一种新的表格格式，用于存储缓慢移动的表格数据。它旨在改进内置在Hive，Presto和Spark中的事实上的标准表布局。



# Spark 接入 Iceberg



## Adding catalogs



### (https://iceberg.apache.org/getting-started/#adding-catalogs）

Iceberg comes with [catalogs](https://iceberg.apache.org/spark#configuring-catalogs) that enable SQL commands to manage tables and load them by name. Catalogs are configured using properties under `spark.sql.catalog.(catalog_name)`.

This command creates a path-based catalog named `local` for tables under `$PWD/warehouse` and adds support for Iceberg tables to Spark’s built-in catalog:

```sql
spark-sql --packages org.apache.iceberg:iceberg-spark3-runtime:0.9.1 \
    --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
    --conf spark.sql.catalog.spark_catalog.type=hive \
    --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \
    --conf spark.sql.catalog.local.type=hadoop \
    --conf spark.sql.catalog.local.warehouse=$PWD/warehouse
```

## Creating a table



To create your first Iceberg table in Spark, use the `spark-sql` shell or `spark.sql(...)` to run a [`CREATE TABLE`](https://iceberg.apache.org/spark#create-table) command:

```sql
-- local is the path-based catalog defined above
CREATE TABLE local.db.table (id bigint, data string) USING iceberg
```

Iceberg catalogs support the full range of SQL DDL commands, including:

- [`CREATE TABLE ... PARTITIONED BY`](https://iceberg.apache.org/spark#create-table)
- [`CREATE TABLE ... AS SELECT`](https://iceberg.apache.org/spark#create-table-as-select)
- [`ALTER TABLE`](https://iceberg.apache.org/spark#alter-table)
- [`DROP TABLE`](https://iceberg.apache.org/spark#drop-table)



## Writing



### (https://iceberg.apache.org/getting-started/#writing)

Once your table is created, insert data using [`INSERT INTO`](https://iceberg.apache.org/spark#insert-into):

```sql
INSERT INTO local.db.table VALUES (1, 'a'), (2, 'b'), (3, 'c');
INSERT INTO local.db.table SELECT id, data FROM source WHERE length(data) = 1;
```

Iceberg supports writing DataFrames using the new [v2 DataFrame write API](https://iceberg.apache.org/spark#writing-with-dataframes):

```sql
spark.table("source").select("id", "data")
     .writeTo("local.db.table").append()
```

The old `write` API is supported, but *not* recommended.

## Reading



### (https://iceberg.apache.org/getting-started/#reading)

To read with SQL, use the an Iceberg table name in a `SELECT` query:

```sql
SELECT count(1) as count, data
FROM local.db.table
GROUP BY data
```

SQL is also the recommended way to [inspect tables](https://iceberg.apache.org/spark#inspecting-tables). To view all of the snapshots in a table, use the `snapshots` metadata table:

```sql
SELECT * FROM local.db.table.snapshots
+-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+
| committed_at            | snapshot_id    | parent_id | operation | manifest_list                                      | ... |
+-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+
| 2019-02-08 03:29:51.215 | 57897183625154 | null      | append    | s3://.../table/metadata/snap-57897183625154-1.avro | ... |
|                         |                |           |           |                                                    | ... |
|                         |                |           |           |                                                    | ... |
| ...                     | ...            | ...       | ...       | ...                                                | ... |
+-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+
```

[DataFrame reads](https://iceberg.apache.org/spark#querying-with-dataframes) are supported and can now reference tables by name using `spark.table`:

```sql
val df = spark.table("local.db.table")
df.count()
```

# Flink  Iceberg代码



```java
public class FlinkCatalog extends AbstractCatalog {

  private final CatalogLoader catalogLoader;
  private final Catalog icebergCatalog;  
  private final String[] baseNamespace;
  private final SupportsNamespaces asNamespaceCatalog;
  private final Closeable closeable;
```

```
Flink Catalog 接口 
// Create the default database if it does not exist.
open


close

listDataBases

databaseExists

createDatabase

listTable(String databaseName)

```

## Iceberg部分

```java

icebergCatalog = cacheEnabled ? CachingCatalog.wrap(originalCatalog) : originalCatalog;

这个 只是 个 CachingCatalog

@Override
  public List<String> listTables(String databaseName) throws DatabaseNotExistException, CatalogException {
    try {
      return icebergCatalog.listTables(toNamespace(databaseName)).stream()
          .map(TableIdentifier::name)
          .collect(Collectors.toList());
    } catch (NoSuchNamespaceException e) {
      throw new DatabaseNotExistException(getName(), databaseName, e);
    }
  }

  @Override
  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {
    Table table = loadIcebergTable(tablePath);
    return toCatalogTable(table);
  }

  Table loadIcebergTable(ObjectPath tablePath) throws TableNotExistException {
    try {
      return icebergCatalog.loadTable(toIdentifier(tablePath));
    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {
      throw new TableNotExistException(getName(), tablePath, e);
    }
  }

  @Override
  public boolean tableExists(ObjectPath tablePath) throws CatalogException {
    return icebergCatalog.tableExists(toIdentifier(tablePath));
  }

  @Override
  public void dropTable(ObjectPath tablePath, boolean ignoreIfNotExists)
      throws TableNotExistException, CatalogException {
    try {
      icebergCatalog.dropTable(toIdentifier(tablePath));
    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {
      throw new TableNotExistException(getName(), tablePath, e);
    }
  }

  @Override
  public void renameTable(ObjectPath tablePath, String newTableName, boolean ignoreIfNotExists)
      throws TableNotExistException, TableAlreadyExistException, CatalogException {
    try {
      icebergCatalog.renameTable(
          toIdentifier(tablePath),
          toIdentifier(new ObjectPath(tablePath.getDatabaseName(), newTableName)));
    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {
      throw new TableNotExistException(getName(), tablePath, e);
    } catch (AlreadyExistsException e) {
      throw new TableAlreadyExistException(getName(), tablePath, e);
    }
  }
```




