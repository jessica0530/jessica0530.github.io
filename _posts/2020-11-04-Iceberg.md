---
layout: post
title: Iceberg
categories: [数据湖]
description: 数据湖
keywords: 数据湖
---

# Iceberg

Apache Iceberg是一种新的表格格式，用于存储缓慢移动的表格数据。它旨在改进内置在Hive，Presto和Spark中的事实上的标准表布局。

设计的初衷更倾向于定义一个标准,开放且通用的数据组织格式,同时屏蔽底层数据存储格式的差异,向上提供统一的操作API,使得不同的引擎可以通过
起提供的API接入



# Spark 接入 Iceberg



## Adding catalogs



### (https://iceberg.apache.org/getting-started/#adding-catalogs）

Iceberg comes with [catalogs](https://iceberg.apache.org/spark#configuring-catalogs) that enable SQL commands to manage tables and load them by name. Catalogs are configured using properties under `spark.sql.catalog.(catalog_name)`.

This command creates a path-based catalog named `local` for tables under `$PWD/warehouse` and adds support for Iceberg tables to Spark’s built-in catalog:

```sql
spark-sql --packages org.apache.iceberg:iceberg-spark3-runtime:0.9.1 \
    --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
    --conf spark.sql.catalog.spark_catalog.type=hive \
    --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \
    --conf spark.sql.catalog.local.type=hadoop \
    --conf spark.sql.catalog.local.warehouse=$PWD/warehouse
```

## Creating a table



To create your first Iceberg table in Spark, use the `spark-sql` shell or `spark.sql(...)` to run a [`CREATE TABLE`](https://iceberg.apache.org/spark#create-table) command:

```sql
-- local is the path-based catalog defined above
CREATE TABLE local.db.table (id bigint, data string) USING iceberg
```

Iceberg catalogs support the full range of SQL DDL commands, including:

- [`CREATE TABLE ... PARTITIONED BY`](https://iceberg.apache.org/spark#create-table)
- [`CREATE TABLE ... AS SELECT`](https://iceberg.apache.org/spark#create-table-as-select)
- [`ALTER TABLE`](https://iceberg.apache.org/spark#alter-table)
- [`DROP TABLE`](https://iceberg.apache.org/spark#drop-table)



## Writing



### (https://iceberg.apache.org/getting-started/#writing)

Once your table is created, insert data using [`INSERT INTO`](https://iceberg.apache.org/spark#insert-into):

```sql
INSERT INTO local.db.table VALUES (1, 'a'), (2, 'b'), (3, 'c');
INSERT INTO local.db.table SELECT id, data FROM source WHERE length(data) = 1;
```

Iceberg supports writing DataFrames using the new [v2 DataFrame write API](https://iceberg.apache.org/spark#writing-with-dataframes):

```sql
spark.table("source").select("id", "data")
     .writeTo("local.db.table").append()
```

The old `write` API is supported, but *not* recommended.

## Reading



### (https://iceberg.apache.org/getting-started/#reading)

To read with SQL, use the an Iceberg table name in a `SELECT` query:

```sql
SELECT count(1) as count, data
FROM local.db.table
GROUP BY data
```

SQL is also the recommended way to [inspect tables](https://iceberg.apache.org/spark#inspecting-tables). To view all of the snapshots in a table, use the `snapshots` metadata table:

```sql
SELECT * FROM local.db.table.snapshots
+-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+
| committed_at            | snapshot_id    | parent_id | operation | manifest_list                                      | ... |
+-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+
| 2019-02-08 03:29:51.215 | 57897183625154 | null      | append    | s3://.../table/metadata/snap-57897183625154-1.avro | ... |
|                         |                |           |           |                                                    | ... |
|                         |                |           |           |                                                    | ... |
| ...                     | ...            | ...       | ...       | ...                                                | ... |
+-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+
```

[DataFrame reads](https://iceberg.apache.org/spark#querying-with-dataframes) are supported and can now reference tables by name using `spark.table`:

```sql
val df = spark.table("local.db.table")
df.count()
```

# Flink  Iceberg代码



```java
public class FlinkCatalog extends AbstractCatalog {

  private final CatalogLoader catalogLoader;
  private final Catalog icebergCatalog;  
  private final String[] baseNamespace;
  private final SupportsNamespaces asNamespaceCatalog;
  private final Closeable closeable;
```

```
Flink Catalog 接口 
// Create the default database if it does not exist.
open


close

listDataBases

databaseExists

createDatabase

listTable(String databaseName)

```

## Iceberg部分

```java

icebergCatalog = cacheEnabled ? CachingCatalog.wrap(originalCatalog) : originalCatalog;

这个 只是 个 CachingCatalog

@Override
  public List<String> listTables(String databaseName) throws DatabaseNotExistException, CatalogException {
    try {
      return icebergCatalog.listTables(toNamespace(databaseName)).stream()
          .map(TableIdentifier::name)
          .collect(Collectors.toList());
    } catch (NoSuchNamespaceException e) {
      throw new DatabaseNotExistException(getName(), databaseName, e);
    }
  }

  @Override
  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {
    Table table = loadIcebergTable(tablePath);
    return toCatalogTable(table);
  }

  Table loadIcebergTable(ObjectPath tablePath) throws TableNotExistException {
    try {
      return icebergCatalog.loadTable(toIdentifier(tablePath));
    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {
      throw new TableNotExistException(getName(), tablePath, e);
    }
  }

  @Override
  public boolean tableExists(ObjectPath tablePath) throws CatalogException {
    return icebergCatalog.tableExists(toIdentifier(tablePath));
  }

  @Override
  public void dropTable(ObjectPath tablePath, boolean ignoreIfNotExists)
      throws TableNotExistException, CatalogException {
    try {
      icebergCatalog.dropTable(toIdentifier(tablePath));
    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {
      throw new TableNotExistException(getName(), tablePath, e);
    }
  }

  @Override
  public void renameTable(ObjectPath tablePath, String newTableName, boolean ignoreIfNotExists)
      throws TableNotExistException, TableAlreadyExistException, CatalogException {
    try {
      icebergCatalog.renameTable(
          toIdentifier(tablePath),
          toIdentifier(new ObjectPath(tablePath.getDatabaseName(), newTableName)));
    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {
      throw new TableNotExistException(getName(), tablePath, e);
    } catch (AlreadyExistsException e) {
      throw new TableAlreadyExistException(getName(), tablePath, e);
    }
  }
```



Iceberg表支持在库模块中组织：

- `iceberg-common` 包含其他模块中使用的实用程序类
- `iceberg-api` 包含公共的Iceberg API
- `iceberg-core`包含Iceberg API的实现并支持Avro数据文件，**这是处理引擎应依赖的东西**
- `iceberg-parquet` 是用于处理由Parquet文件支持的表的可选模块
- `iceberg-arrow` 是用于将Parquet读入Arrow存储器的可选模块
- `iceberg-orc` 是用于处理由ORC文件支持的表的可选模块
- `iceberg-hive-metastore` 是由Hive Metastore Thrift客户端支持的Iceberg表的实现
- `iceberg-data` 是用于直接从JVM应用程序中处理表的可选模块

Iceberg这个项目还具有用于向处理引擎添加Iceberg支持的模块：

- `iceberg-spark2` 是Iceberg 2.4版中Spark的Datasource V2 API的实现（使用iceberg-spark-runtime作为阴影版本）
- `iceberg-spark3` 是Iceberg 3.0版Spark的Datasource V2 API的实现（阴影版本使用iceberg-spark3-runtime）
- `iceberg-flink` 包含用于与Apache Flink集成的类（对阴影版本使用iceberg-flink-runtime）
- `iceberg-mr` 包含一个InputFormat和其他用于与Apache Hive集成的类
- `iceberg-pig` 是Pig的Iceberg的LoadFunc API的实现