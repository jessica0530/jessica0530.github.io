---
layout: post
title: 项目经历整理
categories: [项目经历]
description: 项目经历整理
keywords: 项目经历
---

# Flink特性整理

## 常用算子实现

看基础算子实现篇



## State实现



# Flink vs Spark vs Jstorm

## Spark

Spark Streaming将接收的实时数据流分成一个个的RDD，然后由Spark引擎对RDD做各种处理，其中每个RDD实际是一个小的块数据。所以，Spark Streaming本质上是将流数据分成一段段块数据后，对其进行连续不断的批处理。

### 反压
早期版本的Spark不支持反向压力，但从Spar.1.5版本开始，Spark Streaming引入了反向压力功能。默认情况下，SparkStreaming的反向压力功能是关闭的。当要使用反向压力功能时，需要将spark.streaming.backpressure.enabled设置为True。其工作原理如下。首先，当Spark处理完每批数据时，统计每批数据的处理结束时间、处理时延、等待时延、处理消息数等信息。然后，Spark根据统计信息估计处理速度，并将这个估计值通知给数据生产者。最后，数据生产者根据估计的处理速度，动态调整生产速度，最终使得生产速度与处理速度相匹配
### 状态

在Spark Streaming中，流的状态管理是在部分DStream提供的转化操作中实现的。在流数据状态方面，由于DStream本身将数据流分成RDD做批处理，所以Spark Streaming天然就需要对数据进行缓存和状态管理。换言之，组成DStream的一个个RDD就是一种流数据状态。在DStream上，提供了一些窗口相关的转化API，实现对流数据的窗口管理。在窗口之上还提供了count和reduce两类聚合功能。另外，DStream还提供了union、join和cogroup 3种在多个流之间做关联操作的API。在流信息状态方面，DStream的updateStateByKey操作和mapWithState操作提供了流信息状态管理的方法。updateStateByKey和mapWithState都可以基于key来记录历史信息，并在新的数据到来时对这些信息进行更新。不同的是，updateStateByKey会返回记录的所有历史信息，而mapWithState只会返回处理当前一批数据时更新的信息。就好像，前者返回了一个完整的直方图，而后者只是返回直方图中发生变化的柱条。由此可见，mapWithState比updateStateByKey的性能优越很多。从功能上讲，如果不是用于报表生成的场景，大多数实时流计算应用使用mapWithState会更合适。

### 消息传达可靠性保证

消息传达可靠性保证Spark Streaming对消息可靠性的保证是由数据接收、数据处理和数据输出共同决定的。从1.2版本开始，Spark引入WAL（Write Ahead Logs）机制，可以将接收的数据先保存到错误容忍的存储空间。当开启WAL机制后，再配合可靠的数据接收器（如Kafka），Spark Streaming能够实现“至少一次”的消息接收功能。从1.3版本开始，Spark又引入了Kafka Direct API，进而可以实现“精确一次”的消息接收功能。由于SparkStreaming对数据的处理是基于RDD完成的，而RDD提供了“精确一次”的消息处理功能，所以在数据处理部分，SparkStreaming天然具有“精确一次”的消息可靠性保证机制。但是，Spark Streaming的数据输出部分目前只具有“至少一次”的可靠性保证机制。也就是说，经过处理的数据可能会被多次输出到外部系统。在一些场景下，这么做不会有什么问题。例如，输出数据被保存到文件系统，重复发送的结果只是覆盖之前写过一遍的数据。但是在另一些场景下，如需要根据输出增量更新数据库，那就需要做一些额外的去重处理了。一种可行的方法是，在各个RDD中新增一个唯一标识符来表示这批数据，然后在写入数据库时，使用这个唯一标识符来检查数据之前是否写入过。当然，这时写入数据库的动作需要使用事务来保证一致性


## Storm

### 系统架构图
Storm集群由两种节点组成：Master节点和Worker节点。Master节点运行Nimbus进程，用于代码分发、任务分配和状态监控。Worker节点运行Supervisor进程和Worker进程，其中Supervisor进程负责管理Worker进程的整个生命周期，而Worker进程创建Executor线程，用于执行具体任务（Task）。在Nimbus和Supervisor之间，还需要通过Zookeeper来共享流计算作业状态，协调作业的调度和执行。

### 反压

反向压力Storm支持反向压力。早期版本的Storm通过开启acker机制和max.spout.pending参数实现反向压力。当下游Bolt处理较慢，Spout发送出但没有被确认的消息数超过max.spout.pending参数设定值时，Spout就暂停发送消息。这种方式实现了反向压力，但有一个不算轻微的缺陷。一方面，静态配置max.spout.pending参数很难使得系统在运行时有最佳的反向压力性能表现。另一方面，这种反向压力实现方式本质上只是在消息源头对消息发送速度做限制，而不是对流处理过程中各个阶段做反向压力，它会导致系统的处理速度发生比较严重的抖动，降低系统的运行效率。在较新版本的Storm中，除了监控Spout发送出但没有被确认的消息数外，还需监控每级Bolt接收队列的消息数量。当消息数超过阈值时，通过Zookeeper通知Spout暂停发送消息。这种方式实现了流处理过程中各个阶段反向压力的动态监控，能够更好地在运行时调整对Spout的限速，降低了系统处理速度的抖动，也提高了系统的运行效率

### 流状态 
流的状态前面我们将流的状态分成两种：流数据状态和流信息状态。在流数据状态方面，早期版本的Storm提供了Trident、窗口（window）和自定义批处理3种有状态处理方案。Trident将流数据切分成一个个的元组块（tuple batch），并将其分发到集群中处理。Trident针对元组块的处理，提供了过滤、聚合、关联、分组、自定义函数等功能。其中，聚合、关联、分组等功能在实现过程中涉及状态保存的问题。另外，Trident在元组块处理过程中可能失败，失败后需要重新处理，这个过程涉及状态保存和事务一致性问题。因此，Trident有针对性地提供了一套Trident状态接口（Trident State API）来处理状态和事务一致性问题。Trident支持3种级别的Spout和State：Transactional、Opaque Transactional和No-Transactional。其中，Transactional提供了强一致性保证机制，OpaqueTransactional提供了弱一致性保证机制，No-Transactional未提供一致性保证机制。Storm支持Bolt按窗口处理数据，目前实现的窗口类型包括滑动窗口（sliding window）和滚动窗口（tumbling window）。Storm支持自定义批处理方式。Storm系统内置了定时消息机制，即每隔一段时间向Bolt发送tick元组，Bolt在接收到tick元组后，可以根据需求自行决定什么时候处理数据、处理哪些数据等，在此基础上就可实现各种自定义的批处理方式。例如，可以通过tick实现窗口功能（当然Storm本身已经支持），或实现类似于Flink中watermark的功

从2.0.0版本引入的Stream API提供了window、join、cogroup等流数据状态相关的API，这些API更加通用，使用起来也更方便，在流信息状态方面，早期版本Storm中的Trident状态接口包含对流信息状态的支持，并且还支持了3种级别的事务一致性。例如，使用Trident状态接口可以实现单词计数功能。但是Trident状态与Trident支持的处理功能耦合太紧，这使得Trident状态接口的使用并不通用。例如，在非Trident的Topology中就不能使用Trident状态接口了。所以，当使用Storm做实时流计算时，经常需要用户自行实现对流信息状态的管理。例如，使用Redis来记录事件发生的次数。不过，最新版本Storm的Strea.API已经逐渐开始引入更通用的流信息状态接口，目前提供的updateStateByKey和stateQuery就是这种尝试

### 消息传达可靠性
保证Storm提供了不同级别的消息可靠性保证机制，包括尽力而为（best effort）、至少一次（at least once）和通过Trident实现的精确一次（exactly once）。在Storm中，一条消息被完全处理，是指代表这条消息的元组及由这个元组生成的子元组、孙子元组、各代重孙元组都被成功处理。反之，只要这些元组中有任何一个元组在指定时间内处理失败，那就认为这条消息是处理失败的。不过，要使用Storm的这种消息完全处理机制，需要在程序开发时，配合Storm系统做两件额外的事情。首先，当在处理元组过程中生成了子元组时，需要通过ack告知Storm系统。其次，当完成对一个元组的处理时，也需要通过ack或fail告知Storm系统。在具体业务逻辑开发过程中，用户根据业务需要选择合理的消息保证级别实现即可。很多场景下并非一定要保证严格的数据一致性，毕竟越严格的消息保证级别通常实现起来也会越复杂，性能损耗也会更大。


## Flink

### Flink的系统架构
Flink是一个主从（master/worker）架构的分布式系统。主节点负责调度流计算作业，管理和监控任务执行。当主节点从客户端接收到与作业相关的Jar包和资源后，便对其进行分析和优化，生成执行计划，即需要执行的任务，然后将相关的任务分配给各个从节点，由从节点负责任务的具体执行。Flink可以部署在诸如YARN、Mesos和Kubernetes等分布式资源管理器上，其整体架构与Storm、Spark Streaming等分布式流计算框架类似。与这些流计算框架不同的是，Flink明确地把状态管理（尤其是流信息状态管理）纳入其系统架构中了。[插图]图6-7 Flink的系统架构在Flink节点执行任务的过程中，可以将状态保存到本地，然后通过checkpoint机制，再配合诸如HDFS、S3和NFS这样的分布式文件系统，Flink在不降低性能的同时实现了状态的分布式管理。

### 反向压力
Flink对反向压力的支持非常好，不仅实现了反向压力功能，而且直接内置了反向压力的监控功能。Flink采用有限容量的分布式阻塞队列来进行数据传递，当下游任务从消费队列读取消息的速度过慢时，上游任务往队列中写入消息的速度就非常自然地减慢了。这种反向压力的实现思路和使用JDK自带的BlockingQueue实现反向压力的方法基本一致。值得一提的是，与Storm和Spar.Streaming需要明确打开启动开关才能使用反向压力功能不一样的是，Flink的反向压力功能是其数据传送方案自带的，不需特别再去实现，使用时也无须特别打开启动开关。


### 流的状态
Flink是第一个明确地将流信息状态管理从流数据状态管理剥离出来的流计算框架。大多数流计算框架要么没有流信息状态管理，要么实现的流信息状态管理非常有限，要么流信息状态管理混淆在了流数据状态管理中，使用起来并不方便和明晰。在流数据状态方面，Flink有关流数据状态的管理都集中在DataStream的转化操作上。这是非常合理的，因为流数据状态管理本身属于流转化和管理的一部分。例如，流按窗口分块处理、多流的合并、事件乱序处理等功能的实现虽然也涉及数据缓存和有状态操作，但这些功能原本就应该由流计算引擎来处理。在DataStream中，与窗口管理相关的API包括Window和WindowAll。其中，Window针对的是KeyedStream，而WindowAll针对的是非KeyedStream。在窗口之内，则提供了一系列窗口聚合计算的方法，如Reduce、Fold、Sum、Min、Max和Apply等。DataStream提供了一系列有关流与流之间计算的操作，如Union、Join、CoGroup和Connect等。另外，DataStream还提供了非常有特色的KeyedStream。KeyedStream是指将流按照指定的键值，在逻辑上分成多个独立的流。在计算时，这些逻辑流的状态彼此独立、互不影响，但是在物理上这些独立的流可能合并在同一条物理的数据流中。因此，在KeyedStream具体实现时，Flink会在处理每个消息前将当前运行时上下文切换到键值所指定流的上下文，就像线程栈的切换那样，这样优雅地避免了不同逻辑流在运算时的相互干扰

在流信息状态方面，Flink对流信息状态管理的支持，是其相比当前其他流计算框架更显优势的地方。Flink在DataStream之外提供了独立的状态管理接口。可以说，实现流信息状态管理，并将其从流本身的管理中分离出来，是Flink在洞悉流计算本质后的明智之举。因为，如果DataStream是对数据在时间维度的管理，那么状态接口其实是在空间维度对数据的管理。Flink之前的流数据框架对这两个概念的区分可以说并不是非常明确，这也导致它们关于状态的设计不是非常完善，甚至根本没有。在Flink中，状态接口有两种类型：Keyed State和OperatorState。它们既可以用于流信息状态管理，也可以用于流数据状态管理。
1.Keyed StateKeyed State与KeyedStream相关。KeyedStream是对流按照key值做出的逻辑划分。每个逻辑流都有自己的上下文，就像每个线程都有自己的线程栈一样。当我们需要在逻辑流中记录一些状态信息时，就可以使用Keyed State。例如要实现“统计不同IP上出现的不同设备数”的功能，就可以将流按照IP分成KeyedStream，这样来自不同IP的设备事件会分发到不同IP独有的逻辑流中。然后在逻辑流处理过程中，使用KeyedState来记录不同设备数。如此一来，就非常方便地实现了“统计不同IP上出现的不同设备数”的功能

Operator State Operator State与算子有关。其实与Keyed State的设计思路非常一致，Keyed State是按键值划分状态空间的，而OperatorState是按照算子的并行度划分状态空间的。每个OperatorState绑定到算子的一个并行实例上，因而这些并行实例在执行时可以维护各自的状态。这有点儿像线程局部量，每个线程都维护自己的一个状态对象，在运行时互不影响。例如，当KafkaConsumer在消费同一个主题的不同分区时，可以用OperatorState来维护各自消费分区的偏移量。Flink 1.6版本引入了状态生存时间值（state time-to-live），这为实际开发中淘汰过期的状态提供了极大的便利。不过美中不足的是，Flink虽然针对状态存储提供了TTL机制，但是TTL本身实际是一种非常底层的功能。如果Flink能够针对状态管理提供诸如窗口管理这样的功能，那么Flink的流信息状态管理会更加完善和方便

### 消息传达可靠性
Flink基于snapshot和checkpoint的故障恢复机制，在内部提供了exactly-once的语义。当然，得到这个保证的前提是，在Flink应用中保存状态时必须使用Flink内部的状态机制，如Keyed State和Operator State。因为这些Flink内部状态的保存和恢复方案都包含在Flink的故障恢复机制内，由系统保证了状态的一致性。如果使用不包含在Flink故障恢复机制内的方案存储状态，如用另外独立的Redis记录PV/UV统计状态，那么就不能获得exactly-once级别的可靠性保证，而只能实现at-least-once级别的可靠性保证。要想在Flink中实现从数据流输入到输出之间端到端的exactly-once数据传送，还必须得到Flink connectors配合才行。不同的connectors提供了不同级别的可靠性保证机制。例如，在Source端，Apache Kafka提供了exactly once保证机制，Twitter Streaming API提供了at most once保证机制。在Sink端，HDFS rolling sink提供了exactl.once保证机制，Kafk.producer则只提供了exactly once的保证机制。





# Pulsar和Kafka区别

# 数据仓库



## 实时数仓

![实时数仓](/images/posts/实时数仓.png)



![实时数仓2](/images/posts/实时数仓2.png)

## 离线数仓

![传统数仓](/images/posts/传统数仓.png)



### 实时数仓和传统数仓的对比

实时数仓和传统数仓的对比主要可以从四个方面考虑：

- 第一个是分层方式，离线数仓为了考虑到效率问题，一般会采取空间换时间的方式，层级划分会比较多；则实时数仓考虑到实时性问题，一般分层会比较少，另外也减少了中间流程出错的可能性。
- 第二个是事实数据存储方面，离线数仓会基于 HDFS，实时数仓则会基于消息队列（如 Kafka）。
- 第三个是维度数据存储，实时数仓会将数据放在 KV 存储上面。
- 第四个是数据加工过程，离线数仓一般以 Hive、Spark 等批处理为主，而实时数仓则是基于实时计算引擎如 Storm、Flink 等，以流处理为主。





## 一站式解决方案

![实时数仓3](/images/posts/实时数仓3.png)



# 规则引擎



# Presto Http注册那块

# 推荐系统的架构



# RestFul服务架构设计



# 工程结构



![应用分层](/Users/jessica/Desktop/应用分层.png)

- 开放接口层:可直接封装 Service 方法暴露成 RPC 接口;通过 Web 封装成 http 接口;进行 网关安全控制、流量控制等。 

- 终端显示层:各个端的模板渲染并执行显示的层。当前主要是 velocity 渲染，JS 渲染， JSP 渲染，移动端展示等。 

- Web 层:主要是对访问控制进行转发，各类基本参数校验，或者不复用的业务简单处理等。 

- Service 层:相对具体的业务逻辑服务层。 

- Manager 层:通用业务处理层，它有如下特征:
   1) 对第三方平台封装的层，预处理返回结果及转化异常信息;
   2) 对Service层通用能力的下沉，如缓存方案、中间件通用处理; 

   3) 与DAO层交互，对多个DAO的组合复用。 

- DAO 层:数据访问层，与底层 MySQL、Oracle、Hbase 等进行数据交互。 

- 外部接口或第三方平台:包括其它部门RPC开放接口，基础平台，其它公司的HTTP接口。 



(分层异常处理规约)在 DAO 层，产生的异常类型有很多，无法用细粒度的异常进 行catch，使用catch(Exception e)方式，并throw new DAOException(e)，不需要打印 日志，因为日志在 Manager/Service 层一定需要捕获并打印到日志文件中去，如果同台服务 器再打日志，浪费性能和存储。在 Service 层出现异常时，必须记录出错日志到磁盘，尽可能 带上参数信息，相当于保护案发现场。如果 Manager 层与 Service 同机部署，日志方式与 DAO 层处理一致，如果是单独部署，则采用与 Service 一致的处理方式。Web 层绝不应该继续往上 抛异常，因为已经处于顶层，如果意识到这个异常将导致页面无法正常渲染，那么就应该直接跳转到友好错误页面，加上用户容易理解的错误提示信息。开放接口层要将异常处理成错误码和错误信息方式返回。



## 分层领域模型规约

- DO(Data Object):此对象与数据库表结构一一对应，通过 DAO 层向上传输数据源对象。 

- DTO(Data Transfer Object):数据传输对象，Service 或 Manager 向外传输的对象。 

- BO(Business Object):业务对象，由 Service 层输出的封装业务逻辑的对象。 

- AO(ApplicationObject):应用对象，在Web层与Service层之间抽象的复用对象模型， 极为贴近展示层，复用度不高。 

- VO(View Object):显示层对象，通常是 Web 向模板渲染引擎层传输的对象。 

- Query:数据查询对象，各层接收上层的查询请求。注意超过 2 个参数的查询封装，禁止 

  使用 Map 类来传输。 



## 系统架构设计的目的

1.确定系统边界。确定系统在技术层面上的做与不做。
2.确定系统内模块之间的关系。确定模块之间的依赖关系及模块的宏观输入与输出。
3.确定指导后续设计与演化的原则。使后续的子系统或模块设计在规定的框架内继续演化

4.确定非功能性需求。非功能性需求是指安全性、可用性、可扩展性等。 



 

# LeetCode刷一下基础的



